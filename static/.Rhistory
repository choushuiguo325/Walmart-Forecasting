fit6 = naive(books[,"Hardcover"], h=4)
autoplot(books) +
autolayer(fit5, series = 'Paper', PI=F) +
autolayer(fit6, series = 'Hard', PI=F)
# 7.6
fit7 = holt(books[,"Paperback"], h=4)
summary(fit7)
fit7 = holt(books[,"Paperback"], h=4)
fit8 = holt(books[,"Hardcover"], h=4)
summary(fit8)
summary(fit7)
autoplot(books) +
autolayer(fit7, series = 'Paper', PI=F) +
autolayer(fit8, series = 'Hard', PI=F)
accuracy(fit7)
accuracy(fit8)
# fit7
209.4668 + c(-1,1)*qnorm(.975)*33.4464
# fit8
250.1739 + c(-1,1)*qnorm(.975)*29.2106
fit7$mean
fit9 = rwf(books[,"Paperback"], h=4, drift = T)
fit10 = rwf(books[,"Hardcover"], h=4, drift = T)
fit11 = hw(retails, seasonal = 'mult', damped = F)
autoplot(fit1)
fit11 = hw(retails, seasonal = 'mult', damped = F)
autoplot(fit11)
autoplot(pigs)
autoplot(fitted(ses(pigs, h=4, alpha = 1)))
pigfit = ses(pigs, h=4, alpha = 1)
autoplot(fitted(pigfit))
pigfit = ses(pigs, h=4, alpha = 1)
fit2 = ses(pigs, alpha=.2, initial = "simple")
fit3 = ses(pigs, alpha=.8, initial = "simple")
autoplot(fitted(fit2))
autoplot(fitted(fit3))
autoplot(pigs) +
autolayer(fit2, series = '.2', PI=F) +
autolayer(fit3, series = '.8', PI=F)
fit2 = ses(pigs, h = 20, alpha=.2, initial = "simple")
fit3 = ses(pigs, h = 20, alpha=.8, initial = "simple")
autoplot(pigs) +
autolayer(fit2, series = '.2', PI=F) +
autolayer(fit3, series = '.8', PI=F)
autoplot(pigs) +
autolayer(fit2$fitted, series = '.2', PI=F) +
autolayer(fit3$fitted, series = '.8', PI=F)
library(ggplot2)
library(forecast)
library(fpp2)
library(fma)
install.packages("splitstackshape")
install.packages("tidyverse")
install.packages("devtools")
install.packages("plotly")
install.packages("crosstalk")
install.packages("DT")
install.packages("fivethirtyeight")
# A vector is a basic unit of data structure in R
# All elements in a vector must be of the same type
disney_characters <- c("mickey", "minnie", "donald", "goofy")
presidents <- c("washington", "adams", "jefferson")
numbers_vector <- c(1, 3, 5, 7, 9, 11)
print(disney_characters)
print(presidents)
print(numbers_vector)
# R is a one-index language!
presidents[1]
### Combine vectors into a single vector
combined_vector <- c(disney_characters, presidents)
combined_vector
### A for-loop
for (x in combined_vector) {
print(x)
}
# Operate over an entire vector en masse
numeric_vector <- 1:length(combined_vector)
squared_vector <- numeric_vector**2
print(combined_vector)
print(numeric_vector)
print(squared_vector)
# Part I
students <- c("Abraham", "Beatrice", "Cory", "Dinah", "Eric", "Felicia")
locker_combinations <- function(class){
for (student in class){
combination <- sample(33,3)
print(student)
print(combination)
}
}
locker_combinations(students)
for (student in students){
second_letter <- substr(student,2,2)
if (second_letter == 'e'){
print(student)
combination <- sample(33:66,3)
print(combination)
}
}
length(sample(1:4))
library(dplyr)
# Simple vectors
# Months of the year
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
# Average rainfall/precipitation in NYC during each month
precipitation <- c(3.9, 2.9, 4.1, 3.9, 4.5, 3.5, 4.5, 4.1, 4.0, 3.4, 3.8, 3.6)
print(months)
print(precipitation)
# Assign names to a vector
# Assign months to precipitation as names
names(precipitation) <- months
# Display precipitation
print(precipitation)
# Display names of precipitation
print(names(precipitation))
# Access a single member of precipitation by its name
mar_precipitation <- precipitation["Mar"]
print(mar_precipitation)
# Summary of data
# Display summary data of precipitation
summary(precipitation)
precipitation_summary <- summary(precipitation)
# Access features of a summary
precipitation_summary["Min."]
precipitation_summary["Mean"]
# Use pipe operators to express a sequence of operations
precipitation %>% summary()
# Use double brackets to access only the value
precipitation_summary[["Max."]]
# Standard deviation
# Display the standard deviation
sd(precipitation)
# Round SD to two digits
stdev <- sd(precipitation)
round(stdev, 2)
# Standard deviation and round using the pipe operator
precipitation %>% sd()
3.141592653 %>% round(2)
# A few more methods
# Determine the length of a vector
length(precipitation)
# Display the sum of a vector
yearly_precipitation <- sum(precipitation)
yearly_precipitation
# The same operations, this time using pipes
precipitation %>% length()
yearly_precipitation2 <- precipitation %>% sum()
print(yearly_precipitation2)
library(tidyverse)
library(ggplot2)
library(forecast)
library(fpp2)
library(fma)
library(lmtest)
library(dbplyr)
library(lubridate)
library(seasonal)
food <- read_csv("sales_train_evaluation_2.csv")
food.252 = as.vector(unlist(food[10,2:1942]))
days = length(food.252)
daily_ts = ts(food.252, seasonal.periods= c(7, 365), start=c(2011,29), frequency = 365)
days
setwd("~/Documents/Evanston/NWU/Academic_study/IMC465/IMC_project1/static")
food <- read_csv("sales_train_evaluation_2.csv")
food.252 = as.vector(unlist(food[10,2:1942]))
days = length(food.252)
daily_ts = ts(food.252, seasonal.periods= c(7, 365), start=c(2011,29), frequency = 365)
days
autoplot(hyndsight)
frequency(hyndsight)
end(hyndsight)
start(hyndsight)
hyndsight
food.252 = as.vector(unlist(food[10,2:1942]))
days = length(food.252)
daily_ts = ts(food.252, start=c(1,7), end = c( 2016, 21,1), frequency = 7)
days
daily_ts
days
end(daily_ts)
daily_ts = ts(food.252, start=c(2011,3,7), end = c(2016,21,1), frequency = 7)
end(daily_ts)
start(daily_ts)
hyndsight
daily_ts = ts(food.252, start=c(3,7), end = c(278,1), frequency = 7)
daily_ts
daily_ts = ts(food.252, start=c(1,7), end = c(278,1), frequency = 7)
length(daily_ts)
daily_ts = ts(food.252, start=c(1,7), end = c(279,1), frequency = 7)
length(daily_ts)
end(daily_ts)
autoplot(daily_ts)
daily_train = window(daily_ts, end = c(275,7))
daily_test = window(daily_ts, start = c(276, 1))
daily_train = window(daily_ts, end = c(274,7))
daily_test = window(daily_ts, start = c(275, 1))
fit.hw0 = hw(daily_train)
fit.hw0$model
fit.hw1 = hw(daily_train, seasonal = 'mult', h = 35)
fit.hw2 = hw(daily_train, damped = T, h = 35)
fit.hw3 = hw(daily_train, damped = F, h = 35)
fit.hw1 = hw(daily_train, seasonal = "mult", h = 35)
accuracy(fit.hw0, daily_test)
accuracy(fit.hw2, daily_test)
accuracy(fit.hw3, daily_test)
autoplot(daily_test, series = "observed") +
autolayer(fit.hw0, series = "Optimal HW", PI = F)
daily_ts
daily_train = window(daily_ts, end = 1923)
daily_test = window(daily_ts, start = 1924)
daily_train = subset(daily_ts, end = 1923)
daily_test = subset(daily_ts, start = 1924)
fit.hw1 = hw(daily_train, seasonal = "mult", h = 35)
fit.hw0 = hw(daily_train, h = 35)
fit.hw0$model
# fit.hw1 = hw(daily_train, seasonal = "mult", h = 35)
fit.hw2 = hw(daily_train, damped = T, h = 35)
fit.hw3 = hw(daily_train, damped = F, h = 35)
accuracy(fit.hw0, daily_test)
accuracy(fit.hw2, daily_test)
accuracy(fit.hw3, daily_test)
autoplot(daily_test, series = "observed") +
autolayer(fit.hw2, series = "Optimal HW", PI = F)
fit.hw2
daily_test
daily_test
length(daily_test)
daily_train = subset(daily_ts, end = 1913)
daily_test = subset(daily_ts, start = 1914)
fit.hw0 = hw(daily_train, h = 35)
fit.hw0$model
# fit.hw1 = hw(daily_train, seasonal = "mult", h = 35)
fit.hw2 = hw(daily_train, damped = T, h = 35)
fit.hw3 = hw(daily_train, damped = F, h = 35)
accuracy(fit.hw0, daily_test)
accuracy(fit.hw2, daily_test)
accuracy(fit.hw3, daily_test)
# accuracy(fit.hw0, daily_test)
autoplot(daily_test, series = "observed") +
autolayer(fit.hw2, series = "Optimal HW", PI = F)
fit.hw0 = hw(daily_train, h = 28)
fit.hw0$model
# fit.hw1 = hw(daily_train, seasonal = "mult", h = 35)
fit.hw2 = hw(daily_train, damped = T, h = 28)
fit.hw3 = hw(daily_train, damped = F, h = 28)
accuracy(fit.hw0, daily_test)
accuracy(fit.hw2, daily_test)
accuracy(fit.hw3, daily_test)
# accuracy(fit.hw0, daily_test)
autoplot(daily_test, series = "observed") +
autolayer(fit.hw2, series = "Optimal HW", PI = F)
checkresiduals(fit.hw2)
fit.hw2$model
accuracy(fit.hw2, daily_test)
fit.hw2
fit.hw2.write.csv("Holt-Winters' method")
write.csv(fit.hw2, "Holt-Winters' method")
write_csv(fit.hw2, "Holt-Winters' method")
write_csv(fit.hw2, "Holt-Winters' method")
type(fit.hw2)
typeof(fit.hw2)
write_csv(as.data.frame(fit.hw2), "Holt-Winters' method")
write_csv(fit.hw2, "Holt-Winters' method.csv")
write_csv(as.data.frame(fit.hw2), "Holt-Winters' method.csv")
ggseasonplot(daily_ts)
library(tidyverse)
library(ggplot2)
library(forecast)
library(fpp2)
library(fma)
library(lmtest)
library(dbplyr)
library(lubridate)
library(seasonal)
BoxCox(daily_ts)
BoxCox.lambda(daily_ts)
daily_trainlog = subset(log(daily_ts), end = 1913)
daily_testlog = subset(log(daily_ts), start = 1914)
accuracy(hw(daily_trainlog, damped = T, h = 28), daily_testlog)
accuracy(hw(daily_train, damped = T, h = 28, lambda = 'auto'), daily_test)
autoplot(daily_test, series = "observed") +
autolayer(fit.hw2log, series = "Optimal HW", PI = F)
fit.hw2log = hw(daily_train, damped = T, h = 28, lambda = 'auto')
autoplot(daily_test, series = "observed") +
autolayer(fit.hw2log, series = "Optimal HW", PI = F)
fit.hw2log$model
library(tidyverse)
library(ggplot2)
library(forecast)
library(fpp2)
library(fma)
library(lmtest)
library(dbplyr)
library(lubridate)
library(seasonal)
library(urca)
library(tseries)
library(gridExtra)
food<- read_csv('../data/calendar_factors.csv')
food.252 = as.vector(unlist(food['sales']))
library(tidyverse)
library(ggplot2)
library(forecast)
library(fpp2)
library(fma)
library(lmtest)
library(dbplyr)
library(lubridate)
library(seasonal)
library(urca)
library(tseries)
library(gridExtra)
food<- read_csv('../data/calendar_factors.csv')
food.252 = as.vector(unlist(food['sales']))
# frequency m = 365
foods_sale = ts(food.252, start=c(2011,29), end = c(2016,144), frequency = 365) # 1941 days
foods_train = window(foods_sale, start=c(2011,29), end = c(2016,116) ) # 1913 days 1-1913
foods_test = window(foods_sale, start=c(2016,117), end = c(2016,144) ) # 28 days 1914-1941
# frequency m = 7
daily_ts = ts(food.252, start=c(1,7), end = c(279,1), frequency = 7) # 1-1941: 2011-1-29 ~ 2016-5-22
daily_train = subset(daily_ts, end = 1913) # training set: 1-1913, 2011-1-29 ~ 2016-4-25 c(1,7)
daily_test = subset(daily_ts, start = 1914) # test set: 1913-1941, 2016-4-26 ~ 2016-5-22 c(275,2)
# m=365
autoplot(foods_train) + ggtitle("Daily unit sales (01/19/2011 - 05/12/2016)") + ylab("Figure.1 Daily unit sales") + xlab("Year")
autoplot(decompose(foods_train, type='additive'))+ ggtitle("Figure.2 Additive decomposistion") + xlab("Year") + ylab("Component") # additive decomposition
autoplot(decompose(foods_train, type='mult')) + ggtitle("Multiplicative decomposition") + xlab("Year") + ylab("Component") # multiplicative decomposition
hist(foods_train, main = 'Distribution of daily unit sales', xlab='Daily unit sale')
# m=7
autoplot(daily_train) + ggtitle("Unit sale time plot")
autoplot(decompose(daily_train, type='additive')) # additive decomposition
autoplot(decompose(daily_train, type='mult')) # multiplicative decomposition
hist(daily_train)
autoplot(diff(daily_train, lag=7))
ggsubseriesplot(daily_train) + ggtitle("Unit sales during a week") # unit sale is higher on weekends
summary(foods_train)
# m=365
print(Acf(foods_train))  # non-stationary, as r1 is large and positive, and the ACF drops to zero slowly
summary(ur.kpss(foods_train)) # the test-statistic 3.01 is much larger than the 1pct 0.74, which means that the data is not stationary
summary(ur.kpss(diff(foods_train)))
kpss.test(foods_train) # 0.01
adf.test(foods_train) # 0.01
Box.test(foods_train) # p-value is tiny, reject the null hypothesis that there's no autocorrelation.
Box.test(foods_train,type='Lj') # same as Box-Pierce test
BoxCox.lambda(foods_train) # 0.31
# m=7
BoxCox.lambda(daily_ts) # 0.204
BoxCox.lambda(daily_train) # 0.204
nsdiffs(foods_train) # 0: seasonal differencing is not required 0
ndiffs(foods_train) # 1:  This process of using a sequence of KPSS tests to determine the appropriate number of first differences is carried out by the function ndiffs().As we saw from the KPSS tests above, one difference is required to make the sale data stationary.
nsdiffs(daily_train) # 1
ndiffs(diff(daily_train, lag = 7)) # 0
food_train_df = food[1:1913,]
food_test_df = food[1914:1941,]
food_train_df
round(cor(food_train_df[, c(5,6,7,9,10,11,12)]),2)
fit.lm1 = lm(formula = sales ~ lag(sales) + as.factor(wday) + as.factor(month)+ as.factor(year) + sporting_days + cultrural_days + national_days + religious_days, data = food_train_df)
summary(fit.lm1)
checkresiduals(fit.lm1)
CV(fit.lm1)
autoplot(daily_train, series='Observations') +
autolayer(ts(predict.lm(fit.lm1), frequency = 7), series = 'Linear regression') +
ylab("Unit sales") +
xlab("Week") +
ggtitle("Daily unit sales (linear regression model)")
autoplot(daily_test, series = 'Observations') +
autolayer(ts(predict.lm(fit.lm1, food_test), frequency = 7, start = c(275,2) ), series = 'Linear regression') +
ylab("Unit sales") +
xlab("Week") +
ggtitle("Predicted daily unit sales (linear regression model)")
autoplot(daily_test, series = 'Observations') +
autolayer(ts(predict.lm(fit.lm1, food_test_df), frequency = 7, start = c(275,2) ), series = 'Linear regression') +
ylab("Unit sales") +
xlab("Week") +
ggtitle("Predicted daily unit sales (linear regression model)")
lambda = BoxCox.lambda(daily_train)
fit.tslm1 = tslm(daily_train ~ trend + season)
summary(fit.tslm1)
checkresiduals(fit.tslm1)
CV(fit.tslm1) # 1.022361e+04
autoplot(daily_train, series = 'Observations') +
autolayer(fitted(fit.tslm1), series = 'fitted') +
xlim(50, 100)
autoplot(daily_test, series = 'Observations') +
autolayer(forecast(fit.tslm1, h=28), PI = F, series = 'linear regression model') +
ylab("Predited unit sales") +
xlab("Week") +
ggtitle("Predicted daily unit sales (linear regressiom model)")
fit.tslmfouriser1 = tslm(daily_train ~ trend + fourier(daily_train, K=3))
summary(fit.tslmfouriser1)
checkresiduals(fit.tslmfouriser1)
CV(fit.tslmfouriser1)
fit.ets0 = ets(daily_train) # MAM, 23422.99
fit.ets0
fit.ets1 = ets(daily_train, lambda = 'auto') # (A,N,A)  12437.65
fit.ets1
fit.ets2 = ets(daily_train, lambda = 0) # (A,N,A) 9747.518
fit.ets2
fit.ets3 = ets(daily_train, lambda = 0, damped = TRUE) # (A,Ad,A), 9767.98
fit.ets3
# fit.ets3 = ets(diff(daily_train,lag=7), lambda = 0)
# fit.ets3# no model to be fitted in
fit.hw0 = hw(daily_train,h = 28) # "addtive" 23774
fit.hw0$model
fit.hw1 = hw(daily_train, seasonal = "additive",h = 28) # 23774.33
fit.hw1$model
fit.hw2 = hw(daily_train, seasonal = "mult", h = 28) # 23462.01
fit.hw2$model
fit.hw3 = hw(daily_train, seasonal = "mult", damped = T, h = 28) # 23564.84
fit.hw3$model
fit.hw4 = hw(daily_train, seasonal = "additive", h = 28, lambda = 'auto') # 12442.36
fit.hw4$model
fit.hw5 = hw(daily_train, seasonal = "additive", h = 28, lambda = 0 ) # 9759.407 AAA
fit.hw5$model
# fit.hw6 = hw(daily_train, seasonal = "mult", h = 28, lambda = 0 ) # forbidden combination of mult and lambda
# fit.hw6$model
fit.ets2 %>% forecast(h=28) %>%
autoplot() +
ylab("Unit sales") +
autolayer(fitted(fit.ets2), series = 'ETS(A, N, A)')
fit.ets2 %>% forecast(h=28) %>%
autoplot() +
ylab("Unit sales") +
xlim(275, 280)
fit.ets2 %>% forecast(h=28) %>%
autoplot(PI = F, series = 'ETS(A, N, A)') +
ylab("Unit sales") +
xlim(275, 279) +
autolayer(daily_test) +
autolayer(fit.hw5, series = "additive + log", PI = F)
autoplot(daily_train, series = 'observations') +
autolayer(fit.hw5, PI = F) +
autolayer(fitted(fit.hw5),series = "additive + log")+
ggtitle('Observations and predictions from additive HW model with log transformation')
autoplot(daily_test, series = 'Observations') +
autolayer(fit.hw5, PI = F, series = 'Additive HW model') +
ylab("Predited unit sales") +
xlab("Week") +
ggtitle("Predicted daily unit sales (additive HW model with log transformation")
checkresiduals(fit.hw5)
accuracy(fit.hw5, daily_test) # additive + log
summary(ur.kpss(daily_train))  # not stationary
summary(ur.kpss(diff(daily_train))) # stationary
summary(ur.kpss(diff(daily_train, lag=7))) # stationary
lambda = BoxCox.lambda(daily_train)
autoplot(diff(daily_train, lag=7)) # seasonally differenced data
daily_train %>% ggtsdisplay()
daily_train %>%  diff(lag=7) %>% ggtsdisplay()
daily_train %>%
Arima(c(0,0,1), seasonal = c(0,1,1)) %>%
residuals() %>% ggtsdisplay()
(daily_train %>%
Arima(c(0,0,1), seasonal = c(0,1,1))) # 14786.4
(daily_train %>%
Arima(c(1,0,1), seasonal = c(0,1,1))) # 14663.87
(daily_train %>%
Arima(c(2,0,1), seasonal = c(0,1,1))) # 14577.05
(daily_train %>%
Arima(c(0,0,1), seasonal = c(0,1,2))) # 14786.22
(daily_train %>%
Arima(c(1,0,1), seasonal = c(0,1,2))) # 14662.2
(daily_train %>%
Arima(c(2,0,1), seasonal = c(0,1,2))) # 14568.07
(daily_train %>%
Arima(c(2,0,1), seasonal = c(1,1,2))) # 14556.24 *****
(daily_train %>%
Arima(c(2,0,1), seasonal = c(2,1,2))) # 14559.84
daily_train %>%
Arima(c(2,0,1), seasonal = c(1,1,2)) %>%
residuals() %>% ggtsdisplay()
(fit.arima1 <- daily_train %>%
Arima(c(2,0,1), seasonal = c(1,1,2)))
checkresiduals(fit.arima1)
autoplot(daily_test, series = 'Observations') +
autolayer(forecast(fit.arima1, h=28), series = 'ARIMA', PI=F) +
ggtitle('Predicted daily unit sales (ARIMA(2,0,1)(1,1,2)[7])') +
ylab('Unit sales') +
xlab('Week')
autoplot(daily_test, series = 'Observations') +
autolayer(fitted(fit.arima1)) +
autolayer(forecast(fit.arima1, h=28), series = 'ARIMA', PI=F) +
ggtitle('Daily unit sales (ARIMA(2,0,1)(1,1,2)[7])') +
ylab('Unit sales') +
xlab('Week')
fit.arimalog = auto.arima(daily_train, lambda = 0)
checkresiduals(fit.arimalog)
autoplot(daily_test) +
autolayer(forecast(fit.arimalog, h=28))
arimalog <- forecast(fit.arimalog,h=28)
accuracy(InvBoxCox(as.numeric(arimalog$mean)),0),daily_test)
fit.arimalog = auto.arima(daily_train, lambda = 0)
checkresiduals(fit.arimalog)
autoplot(daily_test) +
autolayer(forecast(fit.arimalog, h=28), PI=F)
fit.arimalog = auto.arima(daily_train)
checkresiduals(fit.arimalog)
autoplot(daily_test, series = 'Observations') +
autolayer(forecast(fit.arimalog, h=28), PI=F, series = 'Auto arima') +
xlab('Week') +
ylab('Unit sales') +
ggtitle('Predicted daily unit sales')
plots <- list()
for (i in seq(3)) {
fit <- auto.arima(daily_train, xreg = fourier(daily_train, K = i),
seasonal = FALSE, lambda = 0)
plots[[i]] <- autoplot(forecast(fit,
xreg=fourier(daily_train, K=i, h=28))) +
xlim(250,280) +
xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
ylab("unit sales")
}
gridExtra::grid.arrange(
plots[[1]],plots[[2]],plots[[3]],nrow=3)
